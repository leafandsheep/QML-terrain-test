{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leafandsheep/QML-terrain-test/blob/main/model_details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to compare device, season and bike type."
      ],
      "metadata": {
        "id": "A78Oa-GAeZci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ivGvRjsj3_a",
        "outputId": "bfbbada6-db04-4a86-e0ec-03e7103c8268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/code')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/code\")\n",
        "### please update document path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4qylWF5lwvt",
        "outputId": "06c86bae-56e3-4938-e624-20071f2e45f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_infer.py\t\t\t   results_iphone_11th_results.csv\n",
            "data_factory\t\t\t   results_iphone_16th_results.csv\n",
            "device_comparison\t\t   results_iphone_billy_results.csv\n",
            "__pycache__\t\t\t   results_iphone_Nst_results.csv\n",
            "results_android_11th_results.csv   tran_update2.py\n",
            "results_android_16th_results.csv   Untitled0.ipynb\n",
            "results_android_billy_results.csv  win_56\n",
            "results_android_Nst_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tran_update2 import VQTransAE"
      ],
      "metadata": {
        "id": "qE6K8elnmaV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration Parameters\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"All configurable parameters\"\"\"\n",
        "\n",
        "    # Data paths\n",
        "    ORIGINAL_DATA_DIR = './dataset/BIKE'\n",
        "    PROCESSED_DATA_DIR = './dataset/BIKE_processed'\n",
        "    OUTPUT_DIR = './vqtransae_results'\n",
        "\n",
        "    # Original model path (optional, for loading pretrained weights)\n",
        "    PRETRAINED_MODEL_PATH = './win_56/best_vqtransae.pth'\n",
        "\n",
        "    # Feature columns\n",
        "    FEATURE_COLUMNS = ['X', 'Y', 'Z', 'G']\n",
        "\n",
        "    # Model parameters\n",
        "    WIN_SIZE = 56        # Window size\n",
        "    IN_DIM = 4           # Input dimension\n",
        "    HIDDEN_DIM = 64      # Hidden layer dimension\n",
        "    LATENT_DIM = 32      # Latent space dimension\n",
        "    CODEBOOK_SIZE = 1024 # Codebook size\n",
        "    D_MODEL = 64         # Transformer dimension\n",
        "    N_HEADS = 4          # Number of attention heads\n",
        "    N_LAYERS = 3         # Number of Transformer layers\n",
        "\n",
        "    # Data parameters\n",
        "    STEP = 5             # Sliding window step\n",
        "    BATCH_SIZE = 64      # Batch size\n",
        "    VAL_RATIO = 0.15     # Validation set ratio\n",
        "\n",
        "    # Training parameters\n",
        "    EPOCHS = 100         # Number of training epochs\n",
        "    LEARNING_RATE = 1e-4 # Learning rate\n",
        "    CODEBOOK_LR = 5e-4   # Codebook learning rate (higher)\n",
        "    WEIGHT_DECAY = 1e-5  # Weight decay\n",
        "\n",
        "    # Regularization parameters (prevent codebook collapse)\n",
        "    ENTROPY_WEIGHT = 0.3       # Entropy regularization weight\n",
        "    ENTROPY_TARGET_RATIO = 0.8 # Target entropy ratio\n",
        "    DIVERSITY_WEIGHT = 0.2     # Diversity regularization weight\n",
        "    VQ_WEIGHT_BASE = 2.0       # VQ loss base weight\n",
        "\n",
        "    # Codebook refresh parameters\n",
        "    REFRESH_EVERY = 5          # Refresh every N epochs\n",
        "    MIN_USAGE_THRESHOLD = 10   # Minimum usage threshold\n",
        "    ACTIVE_TOKEN_TARGET = 100  # Target active token count\n",
        "\n",
        "    # Composite score weights (optimal configuration)\n",
        "    ALPHA = 1.0   # Reconstruction error weight\n",
        "    BETA = 1.0    # Quantization distance weight\n",
        "    GAMMA = -0.5  # Attention weight (negative! anomaly samples have more focused attention)\n",
        "\n",
        "    # Threshold percentile\n",
        "    THRESHOLD_PERCENTILE = 20  # Use 20th percentile of validation set\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "def to_serializable(obj):\n",
        "    \"\"\"Convert numpy/torch objects to JSON serializable format\"\"\"\n",
        "    if isinstance(obj, np.generic):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.detach().cpu().tolist()\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_serializable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [to_serializable(v) for v in obj]\n",
        "    return obj\n",
        "\n",
        "\n",
        "def save_json(data, path: Path, indent: int = 2):\n",
        "    \"\"\"Save JSON file\"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(to_serializable(data), f, indent=indent, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def speed_bins(speeds: np.ndarray, bounds: Tuple = (3.0, 6.0, 9.0)) -> np.ndarray:\n",
        "    \"\"\"Bin speeds into intervals for stratified scaling\"\"\"\n",
        "    if speeds is None or len(speeds) == 0:\n",
        "        return np.zeros(0, dtype=int)\n",
        "    return np.digitize(speeds, bounds, right=False)\n",
        "\n",
        "\n",
        "def robust_scale_by_group(values: np.ndarray, groups: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Robust scaling by group (Median-MAD normalization)\n",
        "\n",
        "    This function is important! It removes baseline differences across speeds.\n",
        "\n",
        "    Formula: scaled = (value - median) / MAD\n",
        "    where MAD = median(|value - median|)\n",
        "    \"\"\"\n",
        "    if values.size == 0:\n",
        "        return values\n",
        "\n",
        "    scaled = np.zeros_like(values, dtype=np.float64)\n",
        "    unique_groups = np.unique(groups) if groups.size else np.array([0])\n",
        "\n",
        "    for gid in unique_groups:\n",
        "        mask = groups == gid if groups.size else np.ones_like(values, dtype=bool)\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        subset = values[mask]\n",
        "        median = np.median(subset)\n",
        "        mad = np.median(np.abs(subset - median)) + 1e-6  # Prevent division by zero\n",
        "        scaled[mask] = (subset - median) / mad\n",
        "\n",
        "    return scaled\n",
        "\n",
        "\n",
        "def token_stats(counts: np.ndarray) -> Tuple[int, float, np.ndarray]:\n",
        "    \"\"\"Compute token usage statistics\"\"\"\n",
        "    total = counts.sum()\n",
        "    if total == 0:\n",
        "        return 0, 0.0, np.zeros_like(counts, dtype=np.float64)\n",
        "\n",
        "    probs = counts.astype(np.float64) / total\n",
        "    non_zero = probs > 0\n",
        "    entropy = -(probs[non_zero] * np.log(probs[non_zero])).sum()\n",
        "    perplexity = float(np.exp(entropy))\n",
        "    active = int((counts > 0).sum())\n",
        "\n",
        "    return active, perplexity, probs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset and DataLoader\n",
        "# ============================================================================\n",
        "\n",
        "class WindowDataset(Dataset):\n",
        "    \"\"\"Sliding window dataset\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, win_size: int, step: int,\n",
        "                 features: List[str] = None):\n",
        "        if features is None:\n",
        "            features = Config.FEATURE_COLUMNS\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.data = df[features].values.astype(np.float32)\n",
        "        self.labels = df['anomaly'].values if 'anomaly' in df.columns else np.zeros(len(df))\n",
        "        self.speeds = df['Speed'].values if 'Speed' in df.columns else np.zeros(len(df))\n",
        "        self.win_size = win_size\n",
        "        self.step = step\n",
        "        self.n_windows = (len(self.data) - win_size) // step + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_windows\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.step\n",
        "        end = start + self.win_size\n",
        "        center = start + self.win_size // 2\n",
        "\n",
        "        window = self.data[start:end]\n",
        "        label = float(np.max(self.labels[start:end]))  # Window is anomaly if any point is anomaly\n",
        "        speed = float(self.speeds[center]) if not np.isnan(self.speeds[center]) else 0.0\n",
        "\n",
        "        return torch.from_numpy(window), torch.tensor(label), torch.tensor(speed)\n",
        "\n",
        "\n",
        "def create_dataloaders(\n",
        "    data_dir: str,\n",
        "    win_size: int = Config.WIN_SIZE,\n",
        "    step: int = Config.STEP,\n",
        "    batch_size: int = Config.BATCH_SIZE\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    \"\"\"Create DataLoaders\"\"\"\n",
        "\n",
        "    data_path = Path(data_dir)\n",
        "\n",
        "    train_dataset = WindowDataset(data_path / 'train.csv', win_size, step)\n",
        "    val_dataset = WindowDataset(data_path / 'val.csv', win_size, step)\n",
        "    test_dataset = WindowDataset(data_path / 'test.csv', win_size, step)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Step 3: Regularization Loss Functions\n",
        "# ============================================================================\n",
        "\n",
        "def entropy_regularizer(batch_counts: torch.Tensor, target_ratio: float,\n",
        "                        device: torch.device) -> torch.Tensor:\n",
        "    if batch_counts.sum() == 0:\n",
        "        return torch.zeros(1, device=device)\n",
        "\n",
        "    probs = batch_counts.float().to(device)\n",
        "    probs = probs / probs.sum()\n",
        "    probs = probs[probs > 0]\n",
        "\n",
        "    entropy = -(probs * probs.log()).sum()\n",
        "    max_entropy = math.log(batch_counts.numel())\n",
        "    target_entropy = target_ratio * max_entropy\n",
        "\n",
        "    return torch.relu(target_entropy - entropy)\n",
        "\n",
        "\n",
        "def diversity_regularizer(indices: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Diversity regularization\n",
        "\n",
        "    Purpose: Penalize overly concentrated token usage\n",
        "    \"\"\"\n",
        "    if indices.numel() == 0:\n",
        "        return torch.zeros(1, device=indices.device)\n",
        "\n",
        "    unique, counts = torch.unique(indices, return_counts=True)\n",
        "    if unique.numel() <= 1:\n",
        "        return torch.ones(1, device=indices.device)\n",
        "\n",
        "    probs = counts.float() / counts.sum()\n",
        "    entropy = -(probs * torch.log(probs + 1e-10)).sum()\n",
        "    target_entropy = torch.log(torch.tensor(float(unique.numel()), device=indices.device))\n",
        "\n",
        "    div_loss = torch.relu((target_entropy - entropy) / (target_entropy + 1e-8))\n",
        "    return div_loss\n",
        "\n",
        "\n",
        "def refresh_inactive_codes(model, usage_counts: np.ndarray, min_usage: int,\n",
        "                           reuse_active: bool = True) -> int:\n",
        "    \"\"\"\n",
        "    Refresh inactive codebook vectors\n",
        "\n",
        "    Strategy: Replace low-usage vectors with active vectors + noise\n",
        "    \"\"\"\n",
        "    if usage_counts.sum() == 0:\n",
        "        return 0\n",
        "\n",
        "    dormant_mask = usage_counts < min_usage\n",
        "    dormant = np.nonzero(dormant_mask)[0]\n",
        "\n",
        "    if dormant.size == 0:\n",
        "        return 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embed = model.quant.embed.weight\n",
        "        scale = 1.0 / math.sqrt(embed.shape[1])\n",
        "        noise = torch.randn((dormant.size, embed.shape[1]), device=embed.device) * scale\n",
        "        dormant_tensor = torch.from_numpy(dormant).to(embed.device)\n",
        "\n",
        "        if reuse_active:\n",
        "            active_idx = np.nonzero(usage_counts >= min_usage)[0]\n",
        "            if active_idx.size > 0:\n",
        "                chosen = np.random.choice(active_idx, size=dormant.size, replace=True)\n",
        "                chosen_tensor = torch.from_numpy(chosen).to(embed.device)\n",
        "                sampled = embed[chosen_tensor]\n",
        "                embed[dormant_tensor] = sampled + 0.1 * noise\n",
        "            else:\n",
        "                embed[dormant_tensor] = noise\n",
        "        else:\n",
        "            embed[dormant_tensor] = noise\n",
        "\n",
        "    return int(dormant.size)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Step 4: Training Functions\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch: int,\n",
        "                entropy_weight: float, entropy_target_ratio: float,\n",
        "                diversity_weight: float, vq_weight: float) -> Dict:\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_recon = 0\n",
        "    total_vq = 0\n",
        "    codebook_size = model.quant.embed.num_embeddings\n",
        "    epoch_hist = torch.zeros(codebook_size, dtype=torch.long)\n",
        "\n",
        "    for x, _, _ in loader:\n",
        "        x = x.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        recon, _, loss_vq, indices = model(x)\n",
        "\n",
        "        # Reconstruction loss\n",
        "        rec_loss = nn.functional.mse_loss(recon, x)\n",
        "        loss = rec_loss + vq_weight * loss_vq\n",
        "\n",
        "        # Token usage statistics\n",
        "        batch_counts = torch.bincount(indices.detach().cpu().flatten(), minlength=codebook_size)\n",
        "        epoch_hist += batch_counts\n",
        "\n",
        "        # Entropy regularization\n",
        "        if entropy_weight > 0:\n",
        "            entropy_reg = entropy_regularizer(batch_counts, entropy_target_ratio, device)\n",
        "            loss = loss + entropy_weight * entropy_reg\n",
        "\n",
        "        # Diversity regularization\n",
        "        if diversity_weight > 0:\n",
        "            div_loss = diversity_regularizer(indices)\n",
        "            loss = loss + diversity_weight * div_loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_recon += rec_loss.item()\n",
        "        total_vq += loss_vq.item()\n",
        "\n",
        "    n_batches = max(1, len(loader))\n",
        "    return {\n",
        "        'loss': total_loss / n_batches,\n",
        "        'recon': total_recon / n_batches,\n",
        "        'vq': total_vq / n_batches,\n",
        "        'token_counts': epoch_hist.numpy()\n",
        "    }\n",
        "\n",
        "\n",
        "def validate(model, loader, device) -> Dict:\n",
        "    \"\"\"Validation\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _, _ in loader:\n",
        "            x = x.to(device)\n",
        "            recon, _, loss_vq, _ = model(x)\n",
        "            rec_loss = nn.functional.mse_loss(recon, x)\n",
        "            loss = rec_loss + loss_vq\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return {'loss': total_loss / max(1, len(loader))}\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    data_dir: str = Config.PROCESSED_DATA_DIR,\n",
        "    output_dir: str = Config.OUTPUT_DIR,\n",
        "    pretrained_path: str = None,\n",
        "    epochs: int = Config.EPOCHS,\n",
        "    config: Config = None\n",
        ") -> Tuple[nn.Module, List[Dict], Path]:\n",
        "    \"\"\"\n",
        "    Train Model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir : str\n",
        "        Preprocessed data directory\n",
        "    output_dir : str\n",
        "        Output directory\n",
        "    pretrained_path : str\n",
        "        Pretrained model path (optional)\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    config : Config\n",
        "        Configuration object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : nn.Module\n",
        "        Trained model\n",
        "    history : List[Dict]\n",
        "        Training history\n",
        "    best_model_path : Path\n",
        "        Path to best model\n",
        "    \"\"\"\n",
        "\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Step 2: Train Model\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    print(\"\\nCreating DataLoaders...\")\n",
        "    train_loader, val_loader, _ = create_dataloaders(\n",
        "        data_dir, config.WIN_SIZE, config.STEP, config.BATCH_SIZE\n",
        "    )\n",
        "    print(f\"   Train: {len(train_loader.dataset)} windows, {len(train_loader)} batches\")\n",
        "    print(f\"   Val: {len(val_loader.dataset)} windows, {len(val_loader)} batches\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing model...\")\n",
        "    from tran_update2 import VQTransAE\n",
        "\n",
        "    model = VQTransAE(\n",
        "        win_size=config.WIN_SIZE,\n",
        "        in_dim=config.IN_DIM,\n",
        "        hidden=config.HIDDEN_DIM,\n",
        "        latent=config.LATENT_DIM,\n",
        "        codebook=config.CODEBOOK_SIZE,\n",
        "        d_model=config.D_MODEL,\n",
        "        heads=config.N_HEADS,\n",
        "        layers=config.N_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    # Re-initialize codebook (critical!)\n",
        "    print(\"   Re-initializing codebook\")\n",
        "    nn.init.xavier_uniform_(model.quant.embed.weight)\n",
        "\n",
        "    # Load pretrained weights (optional)\n",
        "    if pretrained_path and Path(pretrained_path).exists():\n",
        "        print(f\"   Loading pretrained weights: {pretrained_path}\")\n",
        "        checkpoint = torch.load(pretrained_path, map_location=device)\n",
        "        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "\n",
        "        # Only load encoder/decoder, skip codebook\n",
        "        filtered_state = {k: v for k, v in state_dict.items()\n",
        "                         if 'quant.embed' not in k and 'ema' not in k}\n",
        "        model.load_state_dict(filtered_state, strict=False)\n",
        "        print(\"   Loaded encoder/decoder weights (skipped codebook)\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': model.encoder.parameters(), 'lr': config.LEARNING_RATE},\n",
        "        {'params': model.decoder.parameters(), 'lr': config.LEARNING_RATE},\n",
        "        {'params': model.quant.parameters(), 'lr': config.CODEBOOK_LR},\n",
        "        {'params': model.tf_layers.parameters(), 'lr': config.LEARNING_RATE},\n",
        "    ], weight_decay=config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "\n",
        "    # Training history\n",
        "    history = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_active_tokens = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Starting training ({epochs} epochs)\")\n",
        "    print(f\"   Entropy weight: {config.ENTROPY_WEIGHT}\")\n",
        "    print(f\"   Diversity weight: {config.DIVERSITY_WEIGHT}\")\n",
        "    print(f\"   VQ weight: {config.VQ_WEIGHT_BASE}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Adaptive VQ weight\n",
        "        if epoch < 30:\n",
        "            vq_weight = config.VQ_WEIGHT_BASE * 1.5\n",
        "        elif epoch < 60:\n",
        "            vq_weight = config.VQ_WEIGHT_BASE\n",
        "        else:\n",
        "            vq_weight = config.VQ_WEIGHT_BASE * 0.8\n",
        "\n",
        "        # Training\n",
        "        train_result = train_epoch(\n",
        "            model, train_loader, optimizer, device, epoch,\n",
        "            config.ENTROPY_WEIGHT, config.ENTROPY_TARGET_RATIO,\n",
        "            config.DIVERSITY_WEIGHT, vq_weight\n",
        "        )\n",
        "\n",
        "        # Token statistics\n",
        "        active_tokens, perplexity, _ = token_stats(train_result['token_counts'])\n",
        "\n",
        "        # Codebook refresh\n",
        "        refresh_count = 0\n",
        "        if epoch % config.REFRESH_EVERY == 0:\n",
        "            threshold = config.MIN_USAGE_THRESHOLD\n",
        "            if active_tokens < config.ACTIVE_TOKEN_TARGET // 2:\n",
        "                threshold = max(3, threshold // 2)\n",
        "            refresh_count = refresh_inactive_codes(model, train_result['token_counts'], threshold)\n",
        "\n",
        "        # Validation\n",
        "        val_result = validate(model, val_loader, device)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Record\n",
        "        record = {\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_result['loss'],\n",
        "            'train_recon': train_result['recon'],\n",
        "            'train_vq': train_result['vq'],\n",
        "            'val_loss': val_result['loss'],\n",
        "            'active_tokens': active_tokens,\n",
        "            'perplexity': perplexity,\n",
        "            'refresh_count': refresh_count\n",
        "        }\n",
        "        history.append(record)\n",
        "\n",
        "        # Save best model\n",
        "        if active_tokens > best_active_tokens or \\\n",
        "           (active_tokens == best_active_tokens and val_result['loss'] < best_val_loss):\n",
        "            best_val_loss = val_result['loss']\n",
        "            best_active_tokens = active_tokens\n",
        "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        # Print progress\n",
        "        status = f\"Epoch {epoch:3d}: Loss={train_result['loss']:.4f}, Val={val_result['loss']:.4f}, \"\n",
        "        status += f\"Active={active_tokens:4d}/1024, Perplexity={perplexity:.1f}\"\n",
        "        if refresh_count > 0:\n",
        "            status += f\", Refreshed={refresh_count}\"\n",
        "        print(status)\n",
        "\n",
        "        # Periodic save\n",
        "        if epoch % 20 == 0 or epoch == epochs:\n",
        "            save_path = output_path / f'model_epoch_{epoch:03d}.pth'\n",
        "            torch.save({\n",
        "                'state_dict': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'active_tokens': active_tokens,\n",
        "                'perplexity': perplexity,\n",
        "                'hyperparams': {\n",
        "                    'win_size': config.WIN_SIZE,\n",
        "                    'in_dim': config.IN_DIM,\n",
        "                    'hidden': config.HIDDEN_DIM,\n",
        "                    'latent': config.LATENT_DIM,\n",
        "                    'codebook': config.CODEBOOK_SIZE,\n",
        "                    'd_model': config.D_MODEL,\n",
        "                    'heads': config.N_HEADS,\n",
        "                    'layers': config.N_LAYERS\n",
        "                }\n",
        "            }, save_path)\n",
        "            print(f\"   Saved: {save_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Training complete!\")\n",
        "    print(f\"   Best active tokens: {best_active_tokens}/1024\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    best_path = output_path / 'best_model.pth'\n",
        "    torch.save({\n",
        "        'state_dict': best_model_state,\n",
        "        'active_tokens': best_active_tokens,\n",
        "        'hyperparams': {\n",
        "            'win_size': config.WIN_SIZE,\n",
        "            'in_dim': config.IN_DIM,\n",
        "            'hidden': config.HIDDEN_DIM,\n",
        "            'latent': config.LATENT_DIM,\n",
        "            'codebook': config.CODEBOOK_SIZE,\n",
        "            'd_model': config.D_MODEL,\n",
        "            'heads': config.N_HEADS,\n",
        "            'layers': config.N_LAYERS\n",
        "        }\n",
        "    }, best_path)\n",
        "    print(f\"   Best model: {best_path}\")\n",
        "\n",
        "    # Save training history\n",
        "    save_json(history, output_path / 'training_history.json')\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_training_curves(history, output_path / 'training_curves.png')\n",
        "\n",
        "    return model, history, best_path\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Step 5: Evaluation Functions\n",
        "# ============================================================================\n",
        "\n",
        "def compute_scores(model, data_loader, device) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute three component scores\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    scores : Dict\n",
        "        - E: Reconstruction error (higher means more anomalous)\n",
        "        - D: Quantization distance (higher means more anomalous)\n",
        "        - A: Attention dispersion (lower means more anomalous!)\n",
        "        - labels: Ground truth labels\n",
        "        - speeds: Speed values\n",
        "    \"\"\"\n",
        "\n",
        "    recon_errors = []\n",
        "    quant_dists = []\n",
        "    attention_scores = []\n",
        "    all_labels = []\n",
        "    all_speeds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, labels, speeds in data_loader:\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            recon, attn_list, _, indices = model(x)\n",
        "\n",
        "            # 1. Reconstruction error\n",
        "            rec_err = nn.functional.mse_loss(recon, x, reduction='none').mean(dim=(1, 2))\n",
        "            recon_errors.append(rec_err.cpu().numpy())\n",
        "\n",
        "            # 2. Quantization distance\n",
        "            B, T, _ = x.shape\n",
        "            z_e = model.encoder(x)\n",
        "            quantized = model.quant.embed(indices.view(-1)).view_as(z_e)\n",
        "            q_dist = (z_e - quantized).pow(2).sum(dim=2).mean(dim=1)\n",
        "            quant_dists.append(q_dist.cpu().numpy())\n",
        "\n",
        "            # 3. Attention dispersion\n",
        "            attn = attn_list[-1].mean(dim=1).clamp_min(1e-9)\n",
        "            t_dim = attn.shape[-1]\n",
        "            log_t = math.log(max(t_dim, 2))\n",
        "            entropy = -(attn * torch.log(attn)).sum(dim=-1) / (log_t + 1e-9)\n",
        "            gini_like = 1.0 - attn.max(dim=-1).values\n",
        "            composite_attn = 0.5 * (entropy.mean(dim=-1) + gini_like.mean(dim=-1))\n",
        "            attention_scores.append(composite_attn.cpu().numpy())\n",
        "\n",
        "            all_labels.append(labels.numpy())\n",
        "            all_speeds.append(speeds.numpy())\n",
        "\n",
        "    return {\n",
        "        'E': np.nan_to_num(np.concatenate(recon_errors), nan=0.0, posinf=0.0, neginf=0.0),\n",
        "        'D': np.nan_to_num(np.concatenate(quant_dists), nan=0.0, posinf=0.0, neginf=0.0),\n",
        "        'A': np.nan_to_num(np.concatenate(attention_scores), nan=0.0, posinf=0.0, neginf=0.0),\n",
        "        'labels': np.concatenate(all_labels),\n",
        "        'speeds': np.concatenate(all_speeds)\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_composite_score(scores: Dict, alpha: float, beta: float, gamma: float) -> np.ndarray:\n",
        "\n",
        "    groups = speed_bins(scores['speeds'])\n",
        "\n",
        "    E_norm = np.nan_to_num(robust_scale_by_group(scores['E'], groups), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    D_norm = np.nan_to_num(robust_scale_by_group(scores['D'], groups), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    A_norm = np.nan_to_num(robust_scale_by_group(scores['A'], groups), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    composite = alpha * E_norm + beta * D_norm + gamma * A_norm\n",
        "    composite = np.nan_to_num(composite, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    return composite, E_norm, D_norm, A_norm\n",
        "\n",
        "\n",
        "def evaluate_with_threshold(scores: np.ndarray, labels: np.ndarray,\n",
        "                            threshold: float) -> Dict:\n",
        "\n",
        "    preds = (scores > threshold).astype(int)\n",
        "\n",
        "    tp = ((preds == 1) & (labels == 1)).sum()\n",
        "    fp = ((preds == 1) & (labels == 0)).sum()\n",
        "    tn = ((preds == 0) & (labels == 0)).sum()\n",
        "    fn = ((preds == 0) & (labels == 1)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn + 1e-8)\n",
        "    specificity = tn / (tn + fp + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "        'accuracy': float(accuracy),\n",
        "        'specificity': float(specificity),\n",
        "        'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model_path: str,\n",
        "    data_dir: str = Config.PROCESSED_DATA_DIR,\n",
        "    output_dir: str = None,\n",
        "    alpha: float = Config.ALPHA,\n",
        "    beta: float = Config.BETA,\n",
        "    gamma: float = Config.GAMMA,\n",
        "    percentile: int = Config.THRESHOLD_PERCENTILE\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Model\n",
        "\n",
        "    Threshold Selection Principle:\n",
        "    1. Validation set contains only normal samples\n",
        "    2. Compute composite score distribution of validation set\n",
        "    3. Take Nth percentile as threshold\n",
        "    4. Samples with scores above threshold are classified as anomalies\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_path : str\n",
        "        Model path\n",
        "    data_dir : str\n",
        "        Data directory\n",
        "    output_dir : str\n",
        "        Output directory (optional)\n",
        "    alpha, beta, gamma : float\n",
        "        Composite score weights\n",
        "    percentile : int\n",
        "        Threshold percentile\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results : Dict\n",
        "        Evaluation results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Step 3: Evaluate Model\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"   Composite score: S = {alpha}*E_norm + {beta}*D_norm + ({gamma})*A_norm\")\n",
        "    print(f\"   Threshold percentile: {percentile}th percentile\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if output_dir is None:\n",
        "        output_dir = Path(model_path).parent / 'evaluation'\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nDevice: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\nLoading model...\")\n",
        "    from tran_update2 import VQTransAE\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    hyper = checkpoint.get('hyperparams', {\n",
        "        'win_size': Config.WIN_SIZE, 'in_dim': Config.IN_DIM,\n",
        "        'hidden': 64, 'latent': 32, 'codebook': 1024,\n",
        "        'd_model': 64, 'heads': 4, 'layers': 3\n",
        "    })\n",
        "\n",
        "    model = VQTransAE(\n",
        "        hyper['win_size'], hyper['in_dim'],\n",
        "        hidden=hyper['hidden'], latent=hyper['latent'],\n",
        "        codebook=hyper['codebook'], d_model=hyper['d_model'],\n",
        "        heads=hyper['heads'], layers=hyper['layers']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    active_tokens = checkpoint.get('active_tokens', 'N/A')\n",
        "    print(f\"   Active tokens: {active_tokens}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    print(\"\\nCreating DataLoaders...\")\n",
        "    _, val_loader, test_loader = create_dataloaders(data_dir)\n",
        "\n",
        "    print(f\"   Validation windows: {len(val_loader.dataset)}\")\n",
        "    print(f\"   Test windows: {len(test_loader.dataset)}\")\n",
        "\n",
        "    # Compute scores\n",
        "    print(\"\\nComputing scores...\")\n",
        "    val_scores = compute_scores(model, val_loader, device)\n",
        "    test_scores = compute_scores(model, test_loader, device)\n",
        "\n",
        "    # Compute composite scores\n",
        "    val_composite, _, _, _ = compute_composite_score(val_scores, alpha, beta, gamma)\n",
        "    test_composite, E_norm, D_norm, A_norm = compute_composite_score(test_scores, alpha, beta, gamma)\n",
        "    test_labels = test_scores['labels']\n",
        "\n",
        "    # Component separability\n",
        "    print(\"\\nComponent separability (anomaly mean - normal mean):\")\n",
        "    separations = {}\n",
        "    for name, data in [('E', E_norm), ('D', D_norm), ('A', A_norm), ('S', test_composite)]:\n",
        "        normal = data[test_labels == 0]\n",
        "        anomaly = data[test_labels == 1]\n",
        "        sep = anomaly.mean() - normal.mean()\n",
        "        separations[name] = sep\n",
        "        print(f\"   {name}: {sep:+.4f}\")\n",
        "\n",
        "    # Determine threshold - use valid values to compute percentile\n",
        "    valid_val = val_composite[np.isfinite(val_composite)]\n",
        "    if len(valid_val) > 0:\n",
        "        threshold = np.percentile(valid_val, percentile)\n",
        "    else:\n",
        "        threshold = 0.0\n",
        "    print(f\"\\nThreshold: {threshold:.4f} ({percentile}th percentile of validation set)\")\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = evaluate_with_threshold(test_composite, test_labels, threshold)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Evaluation Results\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"   Precision:   {metrics['precision']:.4f}\")\n",
        "    print(f\"   Recall:      {metrics['recall']:.4f}\")\n",
        "    print(f\"   F1 Score:    {metrics['f1']:.4f}\")\n",
        "    print(f\"   Accuracy:    {metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Specificity: {metrics['specificity']:.4f}\")\n",
        "    print(f\"\\n   Confusion Matrix:\")\n",
        "    print(f\"      TP={metrics['tp']}, FP={metrics['fp']}\")\n",
        "    print(f\"      FN={metrics['fn']}, TN={metrics['tn']}\")\n",
        "\n",
        "    # PR-AUC - filter out NaN values\n",
        "    valid_mask = np.isfinite(test_composite) & np.isfinite(test_labels)\n",
        "    valid_composite = test_composite[valid_mask]\n",
        "    valid_labels = test_labels[valid_mask]\n",
        "\n",
        "    if len(valid_labels) > 0 and len(np.unique(valid_labels)) > 1:\n",
        "        pr_auc = average_precision_score(valid_labels, valid_composite)\n",
        "    else:\n",
        "        pr_auc = float('nan')\n",
        "    print(f\"\\n   PR-AUC:  {pr_auc:.4f}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Performance at different percentiles\n",
        "    print(\"\\nPerformance at different percentile thresholds:\")\n",
        "    percentile_results = []\n",
        "    for pct in [10, 15, 20, 25, 30, 40, 50, 60, 70, 80]:\n",
        "        th = np.percentile(val_composite, pct)\n",
        "        m = evaluate_with_threshold(test_composite, test_labels, th)\n",
        "        percentile_results.append({'percentile': pct, 'threshold': th, **m})\n",
        "        print(f\"   {pct:2d}th: F1={m['f1']:.4f}, P={m['precision']:.4f}, R={m['recall']:.4f}\")\n",
        "\n",
        "    # Find best percentile\n",
        "    best_pct_result = max(percentile_results, key=lambda x: x['f1'])\n",
        "    print(f\"\\nBest percentile: {best_pct_result['percentile']}th, F1={best_pct_result['f1']:.4f}\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    plot_evaluation_results(\n",
        "        val_composite, test_composite, test_labels,\n",
        "        threshold, metrics, pr_auc, output_path\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'configuration': {\n",
        "            'formula': f'S = {alpha}*E_norm + {beta}*D_norm + ({gamma})*A_norm',\n",
        "            'alpha': alpha, 'beta': beta, 'gamma': gamma,\n",
        "            'threshold_percentile': percentile,\n",
        "            'threshold_value': float(threshold)\n",
        "        },\n",
        "        'separations': {k: float(v) for k, v in separations.items()},\n",
        "        'metrics': metrics,\n",
        "        'pr_auc': float(pr_auc),\n",
        "        'percentile_results': percentile_results,\n",
        "        'best_percentile_result': best_pct_result,\n",
        "        'model_info': {'active_tokens': active_tokens}\n",
        "    }\n",
        "\n",
        "    save_json(results, output_path / 'evaluation_results.json')\n",
        "\n",
        "    # Save scores\n",
        "    np.savez(\n",
        "        output_path / 'scores.npz',\n",
        "        val_composite=val_composite,\n",
        "        test_composite=test_composite,\n",
        "        test_E_norm=E_norm,\n",
        "        test_D_norm=D_norm,\n",
        "        test_A_norm=A_norm,\n",
        "        test_labels=test_labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nResults saved to: {output_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization Functions\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_curves(history: List[Dict], save_path: Path):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "\n",
        "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    epochs = [h['epoch'] for h in history]\n",
        "    train_loss = [h['train_loss'] for h in history]\n",
        "    val_loss = [h['val_loss'] for h in history]\n",
        "    active_tokens = [h['active_tokens'] for h in history]\n",
        "    perplexity = [h['perplexity'] for h in history]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    axes[0, 0].plot(epochs, train_loss, 'b-', label='Train')\n",
        "    axes[0, 0].plot(epochs, val_loss, 'r-', label='Val')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training & Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(epochs, active_tokens, 'g-', linewidth=2)\n",
        "    axes[0, 1].axhline(Config.ACTIVE_TOKEN_TARGET, color='r', linestyle='--',\n",
        "                       label=f'Target={Config.ACTIVE_TOKEN_TARGET}')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Active Tokens')\n",
        "    axes[0, 1].set_title('Codebook Utilization')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(epochs, perplexity, 'm-', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Perplexity')\n",
        "    axes[1, 0].set_title('Token Distribution Perplexity')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    recon_loss = [h['train_recon'] for h in history]\n",
        "    vq_loss = [h['train_vq'] for h in history]\n",
        "    axes[1, 1].plot(epochs, recon_loss, 'b-', label='Recon')\n",
        "    axes[1, 1].plot(epochs, vq_loss, 'orange', label='VQ')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].set_title('Reconstruction vs VQ Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close()\n",
        "    print(f\"Training curves saved: {save_path}\")\n",
        "\n",
        "\n",
        "def plot_evaluation_results(val_scores, test_scores, test_labels,\n",
        "                            threshold, metrics, pr_auc, output_dir):\n",
        "    \"\"\"Plot evaluation results\"\"\"\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "\n",
        "    # 1. Score distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    test_normal = test_scores[test_labels == 0]\n",
        "    test_anomaly = test_scores[test_labels == 1]\n",
        "\n",
        "    axes[0].hist(val_scores, bins=50, alpha=0.5, label=f'Val (n={len(val_scores)})',\n",
        "                 color='green', density=True)\n",
        "    axes[0].hist(test_normal, bins=50, alpha=0.5, label=f'Test Normal (n={len(test_normal)})',\n",
        "                 color='steelblue', density=True)\n",
        "    axes[0].hist(test_anomaly, bins=50, alpha=0.5, label=f'Test Anomaly (n={len(test_anomaly)})',\n",
        "                 color='coral', density=True)\n",
        "    axes[0].axvline(threshold, color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Threshold={threshold:.2f}')\n",
        "    axes[0].set_xlabel('Composite Score')\n",
        "    axes[0].set_ylabel('Density')\n",
        "    axes[0].set_title('Score Distribution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance metrics\n",
        "    metric_names = ['Precision', 'Recall', 'F1', 'Specificity']\n",
        "    metric_values = [metrics['precision'], metrics['recall'], metrics['f1'], metrics['specificity']]\n",
        "    colors = ['steelblue', 'coral', 'green', 'purple']\n",
        "\n",
        "    bars = axes[1].bar(metric_names, metric_values, color=colors)\n",
        "    axes[1].set_ylim(0, 1.1)\n",
        "    axes[1].set_ylabel('Score')\n",
        "    axes[1].set_title(f'Performance Metrics (F1 = {metrics[\"f1\"]:.4f})')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for bar, val in zip(bars, metric_values):\n",
        "        axes[1].annotate(f'{val:.3f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
        "                        xytext=(0, 3), textcoords='offset points', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path / 'score_distribution.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # 2. PR Curve\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(test_labels, test_scores)\n",
        "    ax.plot(recall_curve, precision_curve, linewidth=2, label=f'PR-AUC = {pr_auc:.4f}')\n",
        "    ax.scatter([metrics['recall']], [metrics['precision']], color='red', s=100, zorder=5,\n",
        "               label=f'Operating Point')\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_title('Precision-Recall Curve')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path / 'pr_curve.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "    cm = np.array([[metrics['tn'], metrics['fp']],\n",
        "                   [metrics['fn'], metrics['tp']]])\n",
        "\n",
        "    im = ax.imshow(cm, cmap='Blues')\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels(['Pred Normal', 'Pred Anomaly'])\n",
        "    ax.set_yticklabels(['True Normal', 'True Anomaly'])\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            ax.text(j, i, cm[i, j], ha='center', va='center',\n",
        "                   color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=16)\n",
        "\n",
        "    plt.colorbar(im)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path / 'confusion_matrix.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Evaluation plots saved to: {output_path}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Function: Complete Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "def run_complete_pipeline(\n",
        "    data_dir: str = Config.PROCESSED_DATA_DIR,\n",
        "    output_dir: str = Config.OUTPUT_DIR,\n",
        "    pretrained_path: str = None,\n",
        "    epochs: int = Config.EPOCHS,\n",
        "    skip_training: bool = False\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Run Complete Pipeline (Training and Evaluation)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir : str\n",
        "        Processed data directory (must contain train.csv, val.csv, test.csv)\n",
        "    output_dir : str\n",
        "        Output directory\n",
        "    pretrained_path : str\n",
        "        Pretrained model path (optional)\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    skip_training : bool\n",
        "        Whether to skip training (use existing model)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results : Dict\n",
        "        Complete results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"VQTransAE Training and Evaluation Pipeline\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"   Data directory: {data_dir}\")\n",
        "    print(f\"   Output directory: {output_dir}\")\n",
        "    print(f\"   Training epochs: {epochs}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    processed_dir = Path(data_dir)\n",
        "\n",
        "    # Step 1: Training\n",
        "    if not skip_training:\n",
        "        model, history, best_model_path = train_model(\n",
        "            data_dir=str(processed_dir),\n",
        "            output_dir=str(output_path / 'models'),\n",
        "            pretrained_path=pretrained_path,\n",
        "            epochs=epochs\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\nSkipping training\")\n",
        "        best_model_path = output_path / 'models' / 'best_model.pth'\n",
        "\n",
        "    # Step 3: Evaluation\n",
        "    results = evaluate_model(\n",
        "        model_path=str(best_model_path),\n",
        "        data_dir=str(processed_dir),\n",
        "        output_dir=str(output_path / 'evaluation'),\n",
        "        alpha=Config.ALPHA,\n",
        "        beta=Config.BETA,\n",
        "        gamma=Config.GAMMA,\n",
        "        percentile=Config.THRESHOLD_PERCENTILE\n",
        "    )\n",
        "\n",
        "    # Generate paper summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Paper Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\"\"\n",
        "VQTransAE model performance on bike road surface anomaly detection:\n",
        "\n",
        "Model Configuration:\n",
        "- Codebook size: {Config.CODEBOOK_SIZE}\n",
        "- Window size: {Config.WIN_SIZE}, Step: {Config.STEP}\n",
        "\n",
        "Composite Anomaly Score:\n",
        "- Formula: S = {Config.ALPHA}*E_norm + {Config.BETA}*D_norm + ({Config.GAMMA})*A_norm\n",
        "- E_norm: Reconstruction error (separability: +{results['separations']['E']:.3f})\n",
        "- D_norm: Quantization distance (separability: +{results['separations']['D']:.3f})\n",
        "- A_norm: Attention dispersion (separability: {results['separations']['A']:.3f})\n",
        "\n",
        "Performance Metrics (@{Config.THRESHOLD_PERCENTILE}th percentile threshold):\n",
        "- F1 Score: {results['metrics']['f1']:.4f}\n",
        "- Precision: {results['metrics']['precision']:.4f}\n",
        "- Recall: {results['metrics']['recall']:.4f}\n",
        "- PR-AUC: {results['pr_auc']:.4f}\n",
        "\n",
        "Best percentile: {results['best_percentile_result']['percentile']}th\n",
        "Best F1: {results['best_percentile_result']['f1']:.4f}\n",
        "\"\"\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\nComplete pipeline finished!\")\n",
        "    print(f\"   All results saved to: {output_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Entry Point\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run complete pipeline using processed data\n",
        "    results = run_complete_pipeline(\n",
        "        data_dir='./vqtransae_results/processed_data',  # Use existing processed data\n",
        "        output_dir='./vqtransae_results',\n",
        "        pretrained_path='./win_56/best_vqtransae.pth',  # Optional: load pretrained weights\n",
        "        epochs=100,\n",
        "        skip_training=False\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdulKMDyOfLu",
        "outputId": "4cfdf376-add3-44a0-ed40-c0f9885555fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VQTransAE Training and Evaluation Pipeline\n",
            "======================================================================\n",
            "   Data directory: ./vqtransae_results/processed_data\n",
            "   Output directory: ./vqtransae_results\n",
            "   Training epochs: 100\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Step 2: Train Model\n",
            "======================================================================\n",
            "Device: cuda\n",
            "\n",
            "Creating DataLoaders...\n",
            "   Train: 2073 windows, 32 batches\n",
            "   Val: 357 windows, 6 batches\n",
            "\n",
            "Initializing model...\n",
            "   Re-initializing codebook\n",
            "   Loading pretrained weights: ./win_56/best_vqtransae.pth\n",
            "   Loaded encoder/decoder weights (skipped codebook)\n",
            "\n",
            "======================================================================\n",
            "Starting training (100 epochs)\n",
            "   Entropy weight: 0.3\n",
            "   Diversity weight: 0.2\n",
            "   VQ weight: 2.0\n",
            "======================================================================\n",
            "Epoch   1: Loss=2.6207, Val=2.1678, Active=  10/1024, Perplexity=1.1\n",
            "Epoch   2: Loss=2.5700, Val=2.1585, Active=   1/1024, Perplexity=1.0\n",
            "Epoch   3: Loss=2.5655, Val=2.1791, Active=   1/1024, Perplexity=1.0\n",
            "Epoch   4: Loss=2.5692, Val=2.1828, Active=   1/1024, Perplexity=1.0\n",
            "Epoch   5: Loss=2.5728, Val=2.1690, Active=   1/1024, Perplexity=1.0, Refreshed=1023\n",
            "Epoch   6: Loss=2.5372, Val=2.1706, Active= 177/1024, Perplexity=1.2\n",
            "Epoch   7: Loss=2.5649, Val=2.1366, Active=   2/1024, Perplexity=1.0\n",
            "Epoch   8: Loss=2.2004, Val=2.0719, Active=   3/1024, Perplexity=2.4\n",
            "Epoch   9: Loss=2.0231, Val=1.9833, Active=   3/1024, Perplexity=3.0\n",
            "Epoch  10: Loss=1.9944, Val=1.9082, Active=   3/1024, Perplexity=3.0, Refreshed=1021\n",
            "Epoch  11: Loss=1.5918, Val=1.5748, Active= 305/1024, Perplexity=11.3\n",
            "Epoch  12: Loss=1.5544, Val=1.2686, Active=  11/1024, Perplexity=8.4\n",
            "Epoch  13: Loss=1.5165, Val=1.1528, Active=  12/1024, Perplexity=8.5\n",
            "Epoch  14: Loss=1.4852, Val=1.0699, Active=  12/1024, Perplexity=8.9\n",
            "Epoch  15: Loss=1.4612, Val=1.0312, Active=  13/1024, Perplexity=9.1, Refreshed=1012\n",
            "Epoch  16: Loss=0.7030, Val=0.7885, Active= 497/1024, Perplexity=91.2\n",
            "Epoch  17: Loss=0.6825, Val=0.6845, Active= 104/1024, Perplexity=79.7\n",
            "Epoch  18: Loss=0.6625, Val=0.6231, Active= 106/1024, Perplexity=79.6\n",
            "Epoch  19: Loss=0.6409, Val=0.5781, Active= 105/1024, Perplexity=80.5\n",
            "Epoch  20: Loss=0.6241, Val=0.5471, Active= 105/1024, Perplexity=81.6, Refreshed=921\n",
            "   Saved: vqtransae_results/models/model_epoch_020.pth\n",
            "Epoch  21: Loss=0.2456, Val=0.4675, Active= 745/1024, Perplexity=329.1\n",
            "Epoch  22: Loss=0.2315, Val=0.4352, Active= 425/1024, Perplexity=304.7\n",
            "Epoch  23: Loss=0.2241, Val=0.4122, Active= 426/1024, Perplexity=299.1\n",
            "Epoch  24: Loss=0.2139, Val=0.4010, Active= 426/1024, Perplexity=297.3\n",
            "Epoch  25: Loss=0.2082, Val=0.3816, Active= 420/1024, Perplexity=294.9, Refreshed=636\n",
            "Epoch  26: Loss=0.1983, Val=0.3664, Active= 760/1024, Perplexity=502.6\n",
            "Epoch  27: Loss=0.1929, Val=0.3498, Active= 704/1024, Perplexity=520.9\n",
            "Epoch  28: Loss=0.1869, Val=0.3400, Active= 694/1024, Perplexity=518.5\n",
            "Epoch  29: Loss=0.1857, Val=0.3290, Active= 695/1024, Perplexity=515.9\n",
            "Epoch  30: Loss=0.1787, Val=0.3223, Active= 684/1024, Perplexity=492.9, Refreshed=372\n",
            "Epoch  31: Loss=0.1761, Val=0.3225, Active= 829/1024, Perplexity=558.8\n",
            "Epoch  32: Loss=0.1729, Val=0.3142, Active= 791/1024, Perplexity=560.5\n",
            "Epoch  33: Loss=0.1717, Val=0.3214, Active= 783/1024, Perplexity=551.3\n",
            "Epoch  34: Loss=0.1688, Val=0.3150, Active= 786/1024, Perplexity=549.2\n",
            "Epoch  35: Loss=0.1666, Val=0.3104, Active= 781/1024, Perplexity=546.2, Refreshed=257\n",
            "Epoch  36: Loss=0.1635, Val=0.3117, Active= 885/1024, Perplexity=597.3\n",
            "Epoch  37: Loss=0.1632, Val=0.3001, Active= 846/1024, Perplexity=602.2\n",
            "Epoch  38: Loss=0.1602, Val=0.2914, Active= 843/1024, Perplexity=601.9\n",
            "Epoch  39: Loss=0.1584, Val=0.2980, Active= 845/1024, Perplexity=604.4\n",
            "Epoch  40: Loss=0.1574, Val=0.2942, Active= 845/1024, Perplexity=606.2, Refreshed=194\n",
            "   Saved: vqtransae_results/models/model_epoch_040.pth\n",
            "Epoch  41: Loss=0.1529, Val=0.2587, Active= 911/1024, Perplexity=625.6\n",
            "Epoch  42: Loss=0.1523, Val=0.2586, Active= 881/1024, Perplexity=629.1\n",
            "Epoch  43: Loss=0.1492, Val=0.2603, Active= 883/1024, Perplexity=626.9\n",
            "Epoch  44: Loss=0.1481, Val=0.2637, Active= 881/1024, Perplexity=631.3\n",
            "Epoch  45: Loss=0.1477, Val=0.2521, Active= 886/1024, Perplexity=635.8, Refreshed=153\n",
            "Epoch  46: Loss=0.1469, Val=0.2500, Active= 942/1024, Perplexity=663.1\n",
            "Epoch  47: Loss=0.1437, Val=0.2530, Active= 929/1024, Perplexity=673.2\n",
            "Epoch  48: Loss=0.1442, Val=0.2501, Active= 933/1024, Perplexity=674.2\n",
            "Epoch  49: Loss=0.1426, Val=0.2473, Active= 933/1024, Perplexity=679.9\n",
            "Epoch  50: Loss=0.1418, Val=0.2510, Active= 932/1024, Perplexity=679.6, Refreshed=105\n",
            "Epoch  51: Loss=0.1405, Val=0.2430, Active= 971/1024, Perplexity=711.9\n",
            "Epoch  52: Loss=0.1394, Val=0.2447, Active= 971/1024, Perplexity=706.5\n",
            "Epoch  53: Loss=0.1386, Val=0.2412, Active= 971/1024, Perplexity=711.6\n",
            "Epoch  54: Loss=0.1373, Val=0.2407, Active= 971/1024, Perplexity=715.7\n",
            "Epoch  55: Loss=0.1383, Val=0.2366, Active= 970/1024, Perplexity=716.1, Refreshed=64\n",
            "Epoch  56: Loss=0.1359, Val=0.2412, Active= 991/1024, Perplexity=738.2\n",
            "Epoch  57: Loss=0.1356, Val=0.2391, Active= 991/1024, Perplexity=739.3\n",
            "Epoch  58: Loss=0.1339, Val=0.2371, Active= 991/1024, Perplexity=733.0\n",
            "Epoch  59: Loss=0.1343, Val=0.2382, Active= 991/1024, Perplexity=738.1\n",
            "Epoch  60: Loss=0.1324, Val=0.2386, Active= 991/1024, Perplexity=731.8, Refreshed=49\n",
            "   Saved: vqtransae_results/models/model_epoch_060.pth\n",
            "Epoch  61: Loss=0.1313, Val=0.2352, Active=1004/1024, Perplexity=728.1\n",
            "Epoch  62: Loss=0.1315, Val=0.2360, Active=1004/1024, Perplexity=728.0\n",
            "Epoch  63: Loss=0.1301, Val=0.2381, Active=1004/1024, Perplexity=733.6\n",
            "Epoch  64: Loss=0.1285, Val=0.2386, Active=1004/1024, Perplexity=731.3\n",
            "Epoch  65: Loss=0.1289, Val=0.2389, Active=1004/1024, Perplexity=724.3, Refreshed=38\n",
            "Epoch  66: Loss=0.1276, Val=0.2367, Active=1012/1024, Perplexity=731.5\n",
            "Epoch  67: Loss=0.1281, Val=0.2399, Active=1012/1024, Perplexity=728.5\n",
            "Epoch  68: Loss=0.1277, Val=0.2343, Active=1012/1024, Perplexity=728.3\n",
            "Epoch  69: Loss=0.1275, Val=0.2380, Active=1011/1024, Perplexity=734.9\n",
            "Epoch  70: Loss=0.1266, Val=0.2367, Active=1012/1024, Perplexity=732.3, Refreshed=32\n",
            "Epoch  71: Loss=0.1270, Val=0.2348, Active=1017/1024, Perplexity=743.4\n",
            "Epoch  72: Loss=0.1262, Val=0.2312, Active=1017/1024, Perplexity=743.6\n",
            "Epoch  73: Loss=0.1260, Val=0.2341, Active=1017/1024, Perplexity=742.1\n",
            "Epoch  74: Loss=0.1252, Val=0.2360, Active=1017/1024, Perplexity=741.6\n",
            "Epoch  75: Loss=0.1247, Val=0.2363, Active=1017/1024, Perplexity=740.4, Refreshed=19\n",
            "Epoch  76: Loss=0.1251, Val=0.2332, Active=1021/1024, Perplexity=743.8\n",
            "Epoch  77: Loss=0.1253, Val=0.2341, Active=1019/1024, Perplexity=741.5\n",
            "Epoch  78: Loss=0.1231, Val=0.2330, Active=1020/1024, Perplexity=740.7\n",
            "Epoch  79: Loss=0.1231, Val=0.2320, Active=1020/1024, Perplexity=743.0\n",
            "Epoch  80: Loss=0.1239, Val=0.2330, Active=1019/1024, Perplexity=744.1, Refreshed=18\n",
            "   Saved: vqtransae_results/models/model_epoch_080.pth\n",
            "Epoch  81: Loss=0.1236, Val=0.2322, Active=1022/1024, Perplexity=742.7\n",
            "Epoch  82: Loss=0.1238, Val=0.2316, Active=1022/1024, Perplexity=744.9\n",
            "Epoch  83: Loss=0.1233, Val=0.2319, Active=1021/1024, Perplexity=746.7\n",
            "Epoch  84: Loss=0.1232, Val=0.2301, Active=1021/1024, Perplexity=746.8\n",
            "Epoch  85: Loss=0.1231, Val=0.2318, Active=1021/1024, Perplexity=749.6, Refreshed=15\n",
            "Epoch  86: Loss=0.1235, Val=0.2316, Active=1022/1024, Perplexity=755.2\n",
            "Epoch  87: Loss=0.1219, Val=0.2305, Active=1022/1024, Perplexity=754.5\n",
            "Epoch  88: Loss=0.1222, Val=0.2337, Active=1022/1024, Perplexity=755.9\n",
            "Epoch  89: Loss=0.1217, Val=0.2323, Active=1022/1024, Perplexity=756.6\n",
            "Epoch  90: Loss=0.1216, Val=0.2333, Active=1022/1024, Perplexity=759.0, Refreshed=11\n",
            "Epoch  91: Loss=0.1214, Val=0.2324, Active=1023/1024, Perplexity=762.3\n",
            "Epoch  92: Loss=0.1218, Val=0.2316, Active=1023/1024, Perplexity=764.3\n",
            "Epoch  93: Loss=0.1211, Val=0.2307, Active=1023/1024, Perplexity=764.2\n",
            "Epoch  94: Loss=0.1209, Val=0.2296, Active=1023/1024, Perplexity=767.6\n",
            "Epoch  95: Loss=0.1205, Val=0.2288, Active=1023/1024, Perplexity=769.8, Refreshed=11\n",
            "Epoch  96: Loss=0.1211, Val=0.2295, Active=1023/1024, Perplexity=770.8\n",
            "Epoch  97: Loss=0.1209, Val=0.2283, Active=1023/1024, Perplexity=772.0\n",
            "Epoch  98: Loss=0.1205, Val=0.2278, Active=1023/1024, Perplexity=774.4\n",
            "Epoch  99: Loss=0.1203, Val=0.2279, Active=1023/1024, Perplexity=776.0\n",
            "Epoch 100: Loss=0.1200, Val=0.2279, Active=1023/1024, Perplexity=777.8, Refreshed=8\n",
            "   Saved: vqtransae_results/models/model_epoch_100.pth\n",
            "\n",
            "======================================================================\n",
            "Training complete!\n",
            "   Best active tokens: 1023/1024\n",
            "======================================================================\n",
            "   Best model: vqtransae_results/models/best_model.pth\n",
            "Training curves saved: vqtransae_results/models/training_curves.png\n",
            "\n",
            "======================================================================\n",
            "Step 3: Evaluate Model\n",
            "======================================================================\n",
            "   Composite score: S = 1.0*E_norm + 1.0*D_norm + (-0.5)*A_norm\n",
            "   Threshold percentile: 20th percentile\n",
            "======================================================================\n",
            "\n",
            "Device: cuda\n",
            "\n",
            "Loading model...\n",
            "   Active tokens: 1023\n",
            "\n",
            "Creating DataLoaders...\n",
            "   Validation windows: 357\n",
            "   Test windows: 1541\n",
            "\n",
            "Computing scores...\n",
            "\n",
            "Component separability (anomaly mean - normal mean):\n",
            "   E: +2.8615\n",
            "   D: +1.8827\n",
            "   A: +187.3102\n",
            "   S: -88.9109\n",
            "\n",
            "Threshold: -2.0201 (20th percentile of validation set)\n",
            "\n",
            "======================================================================\n",
            "Evaluation Results\n",
            "======================================================================\n",
            "   Precision:   0.9553\n",
            "   Recall:      0.8228\n",
            "   F1 Score:    0.8841\n",
            "   Accuracy:    0.7962\n",
            "   Specificity: 0.3412\n",
            "\n",
            "   Confusion Matrix:\n",
            "      TP=1198, FP=56\n",
            "      FN=258, TN=29\n",
            "\n",
            "   PR-AUC:  0.9017\n",
            "======================================================================\n",
            "\n",
            "Performance at different percentile thresholds:\n",
            "   10th: F1=0.9208, P=0.9490, R=0.8942\n",
            "   15th: F1=0.8978, P=0.9502, R=0.8510\n",
            "   20th: F1=0.8841, P=0.9553, R=0.8228\n",
            "   25th: F1=0.8642, P=0.9551, R=0.7891\n",
            "   30th: F1=0.8248, P=0.9523, R=0.7273\n",
            "   40th: F1=0.7604, P=0.9468, R=0.6353\n",
            "   50th: F1=0.6487, P=0.9353, R=0.4966\n",
            "   60th: F1=0.5902, P=0.9345, R=0.4313\n",
            "   70th: F1=0.4472, P=0.9208, R=0.2953\n",
            "   80th: F1=0.3426, P=0.9006, R=0.2115\n",
            "\n",
            "Best percentile: 10th, F1=0.9208\n",
            "\n",
            "Generating visualizations...\n",
            "Evaluation plots saved to: vqtransae_results/evaluation\n",
            "\n",
            "Results saved to: vqtransae_results/evaluation\n",
            "\n",
            "======================================================================\n",
            "Paper Summary\n",
            "======================================================================\n",
            "\n",
            "VQTransAE model performance on bike road surface anomaly detection:\n",
            "\n",
            "Model Configuration:\n",
            "- Codebook size: 1024\n",
            "- Window size: 56, Step: 5\n",
            "\n",
            "Composite Anomaly Score:\n",
            "- Formula: S = 1.0*E_norm + 1.0*D_norm + (-0.5)*A_norm\n",
            "- E_norm: Reconstruction error (separability: +2.861)\n",
            "- D_norm: Quantization distance (separability: +1.883)\n",
            "- A_norm: Attention dispersion (separability: 187.310)\n",
            "\n",
            "Performance Metrics (@20th percentile threshold):\n",
            "- F1 Score: 0.8841\n",
            "- Precision: 0.9553\n",
            "- Recall: 0.8228\n",
            "- PR-AUC: 0.9017\n",
            "\n",
            "Best percentile: 10th\n",
            "Best F1: 0.9208\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Complete pipeline finished!\n",
            "   All results saved to: vqtransae_results\n"
          ]
        }
      ]
    }
  ]
}